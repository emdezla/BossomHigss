{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python notebook used to tune the model for the Higgs Boson Challenge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EPFL - Machine Learning - Autumn 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from proj1_helpers import *\n",
    "from implementations import *\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Randomisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=374534\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = '../Data/train.csv' # TODO: download train data and supply path here \n",
    "DATA_TEST_PATH = '../Data/test.csv' # TODO: download train data and supply path here \n",
    "\n",
    "y_train_raw, x_train_raw, ids_train_raw = load_csv_data(DATA_TRAIN_PATH,sub_sample=True)\n",
    "_, x_test_raw, ids_test_raw = load_csv_data(DATA_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of x_train are  (5000, 30)\n",
      "The dimension of y_train is  (5000,)\n",
      "The dimension of ids_train is  (5000,) \n",
      "\n",
      "The dimensions of x_test are  (568238, 30)\n",
      "The dimension of ids_test is  (568238,)\n"
     ]
    }
   ],
   "source": [
    "y_train = np.copy(y_train_raw)\n",
    "x_train = np.copy(x_train_raw)\n",
    "ids_train = np.copy(ids_train_raw)\n",
    "x_test = np.copy(x_test_raw)\n",
    "ids_test = np.copy(ids_test_raw)\n",
    "\n",
    "print(\"The dimensions of x_train are \",x_train.shape)\n",
    "print(\"The dimension of y_train is \",y_train.shape)\n",
    "print(\"The dimension of ids_train is \",ids_train.shape, \"\\n\")\n",
    "print(\"The dimensions of x_test are \",x_test.shape)\n",
    "print(\"The dimension of ids_test is \",ids_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Functions needed to perform the model tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip to part 4) for the actual model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_k_indices(y, k_fold, seed):\n",
    "    \"\"\"build k indices for k-fold cross-validation.\"\"\"\n",
    "    num_row = y.shape[0]\n",
    "    interval = int(num_row / k_fold)\n",
    "    np.random.seed(seed)\n",
    "    indices = np.random.permutation(num_row)\n",
    "    k_indices = [indices[k * interval: (k + 1) * interval] for k in range(k_fold)]\n",
    "    return np.array(k_indices)\n",
    "\n",
    "def cross_validation(y, x, k_indices, k, lambda_, degrees, method, initial_w, max_iters, gamma):\n",
    "    \"\"\"return the accuracy of given method.\"\"\"\n",
    "    y_test=y[k_indices[k,:]]\n",
    "    x_test=x[k_indices[k,:]]   \n",
    "    y_train=np.delete(y,k)\n",
    "    x_train=np.delete(x,k,0)\n",
    "    \n",
    "    y_pred_train, y_pred_test = prediction(x_train, y_train, x_test, degrees, method, initial_w, max_iters, gamma, lambda_)\n",
    "    \n",
    "    accuracy_train, F1_train = check_accuracy(y_pred_train, y_train)\n",
    "    accuracy_test, F1_test = check_accuracy(y_pred_test, y_test)\n",
    "    return accuracy_train, accuracy_test, F1_train, F1_test\n",
    "\n",
    "def cross_validation_visualization(lambdas, acc_tr, acc_te, f1_tr, f1_te):\n",
    "    \"\"\"visualization of the accuracy and the f1 score for the train data and the test data.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(12,4)\n",
    "    ax_acc = fig.add_subplot(1, 2, 1)\n",
    "    ax_f1 = fig.add_subplot(1, 2, 2)\n",
    "    \n",
    "    ax_acc.set_xlabel('lambda')\n",
    "    ax_acc.set_ylabel('accuracy')\n",
    "    ax_acc.semilogx(lambdas, acc_tr, marker=\".\", color='b', label='train accuracy')\n",
    "    ax_acc.semilogx(lambdas, acc_te, marker=\".\", color='r', label='test accuracy')\n",
    "    ax_acc.set_title('Accuracy')           \n",
    "    ax_acc.grid(True)\n",
    "    ax_acc.legend(loc=2)\n",
    "    \n",
    "    ax_f1.set_xlabel('lambda')\n",
    "    ax_f1.set_ylabel('f1 score')\n",
    "    ax_f1.semilogx(lambdas, f1_tr, marker=\".\", color='b', label='train f1 score')\n",
    "    ax_f1.semilogx(lambdas, f1_te, marker=\".\", color='r', label='test f1 score')\n",
    "    ax_f1.set_title('F1 score')           \n",
    "    ax_f1.grid(True)\n",
    "    ax_f1.legend(loc=2)\n",
    "    \n",
    "    fig.savefig('cross_validation')\n",
    "\n",
    "\n",
    "def cross_validation_demo(y, x, k_fold, lambdas, degrees,seed=1, method=\"RLR\", initial_w=None,\n",
    "               max_iters=10000, gamma=1e-10):\n",
    "    \"\"\"to do\"\"\"\n",
    "    k_indices = build_k_indices(y, k_fold,seed)\n",
    "    acc_tr = []\n",
    "    acc_te = []\n",
    "    f1_tr = []\n",
    "    f1_te = []\n",
    "    for lambda_ in lambdas:\n",
    "        acc_tr_lambda=0;\n",
    "        acc_te_lambda=0;\n",
    "        f1_tr_lambda=0;\n",
    "        f1_te_lambda=0;\n",
    "        for k in range(k_fold):\n",
    "            accuracy_train, accuracy_test, f1_train, f1_test = cross_validation(y, x, k_indices, k, lambda_, degrees, method, initial_w, max_iters, gamma)\n",
    "            \n",
    "            acc_tr_lambda += accuracy_train/k_fold\n",
    "            acc_te_lambda += accuracy_test/k_fold\n",
    "            f1_tr_lambda += f1_train/k_fold\n",
    "            f1_te_lambda += f1_test/k_fold\n",
    "            \n",
    "        acc_tr.append(acc_tr_lambda)\n",
    "        acc_te.append(acc_te_lambda)\n",
    "        f1_tr.append(f1_tr_lambda)\n",
    "        f1_te.append(f1_te_lambda)\n",
    "       \n",
    "    cross_validation_visualization(lambdas, acc_tr, acc_te, f1_tr, f1_te)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dimensions of x_train are  (5000, 30)\n",
      "The dimension of y_train is  (5000,)\n",
      "The dimension of ids_train is  (5000,) \n",
      "\n",
      "The dimensions of x_test are  (568238, 30)\n",
      "The dimension of ids_test is  (568238,)\n"
     ]
    }
   ],
   "source": [
    "y_train, x_train, ids_train = data_preprocessing(y_train, x_train, ids_train,\"mean\")\n",
    "_, x_test, ids_test = data_preprocessing(_, x_test, ids_test,\"mean\")\n",
    "\n",
    "print(\"The dimensions of x_train are \",x_train.shape)\n",
    "print(\"The dimension of y_train is \",y_train.shape)\n",
    "print(\"The dimension of ids_train is \",ids_train.shape, \"\\n\")\n",
    "print(\"The dimensions of x_test are \",x_test.shape)\n",
    "print(\"The dimension of ids_test is \",ids_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.333543959664\n",
      "Current iteration=2000, loss=3449.670104820748\n",
      "Current iteration=3000, loss=3449.0309818277606\n",
      "Current iteration=4000, loss=3448.7106380060613\n",
      "Current iteration=5000, loss=3448.5170833410757\n",
      "Current iteration=6000, loss=3448.380817283923\n",
      "Current iteration=7000, loss=3448.272791022628\n",
      "Current iteration=8000, loss=3448.1797655212713\n",
      "Current iteration=9000, loss=3448.0953504686217\n",
      "Current iteration=10000, loss=3448.0163302085493\n",
      "Current iteration=11000, loss=3447.9410307409808\n",
      "Current iteration=12000, loss=3447.868551842032\n",
      "Current iteration=13000, loss=3447.798391168664\n",
      "Current iteration=14000, loss=3447.73025481004\n",
      "Current iteration=15000, loss=3447.6639597487774\n",
      "Current iteration=16000, loss=3447.599382844988\n",
      "Current iteration=17000, loss=3447.5364338372783\n",
      "Current iteration=18000, loss=3447.475040925501\n",
      "Current iteration=19000, loss=3447.4151430172537\n",
      "Current iteration=20000, loss=3447.3566855325384\n",
      "Current iteration=21000, loss=3447.2996181197714\n",
      "Current iteration=22000, loss=3447.2438934029283\n",
      "Current iteration=23000, loss=3447.1894662865748\n",
      "Current iteration=24000, loss=3447.13629356323\n",
      "Current iteration=25000, loss=3447.0843336846356\n",
      "Current iteration=26000, loss=3447.033546621803\n",
      "Current iteration=27000, loss=3446.9838937730387\n",
      "Current iteration=28000, loss=3446.935337897801\n",
      "Current iteration=29000, loss=3446.887843064402\n",
      "Current iteration=30000, loss=3446.8413746050996\n",
      "Current iteration=31000, loss=3446.795899075138\n",
      "Current iteration=32000, loss=3446.7513842139397\n",
      "Current iteration=33000, loss=3446.7077989075283\n",
      "Current iteration=34000, loss=3446.665113151754\n",
      "Current iteration=35000, loss=3446.6232980161285\n",
      "Current iteration=36000, loss=3446.582325608225\n",
      "Current iteration=37000, loss=3446.542169038646\n",
      "Current iteration=38000, loss=3446.5028023866103\n",
      "Current iteration=39000, loss=3446.4642006661943\n",
      "Current iteration=40000, loss=3446.4263397932905\n",
      "Current iteration=41000, loss=3446.3891965533276\n",
      "Current iteration=42000, loss=3446.352748569777\n",
      "Current iteration=43000, loss=3446.3169742735\n",
      "Current iteration=44000, loss=3446.2818528729345\n",
      "Current iteration=45000, loss=3446.247364325157\n",
      "Current iteration=46000, loss=3446.213489307825\n",
      "Current iteration=47000, loss=3446.1802091919967\n",
      "Current iteration=48000, loss=3446.147506015842\n",
      "Current iteration=49000, loss=3446.115362459227\n",
      "Current iteration=50000, loss=3446.083761819177\n",
      "Current iteration=51000, loss=3446.0526879861936\n",
      "Current iteration=52000, loss=3446.022125421423\n",
      "Current iteration=53000, loss=3445.992059134656\n",
      "Current iteration=54000, loss=3445.9624746631375\n",
      "Current iteration=55000, loss=3445.9333580511766\n",
      "Current iteration=56000, loss=3445.9046958305225\n",
      "Current iteration=57000, loss=3445.8764750015025\n",
      "Current iteration=58000, loss=3445.848683014884\n",
      "Current iteration=59000, loss=3445.821307754448\n",
      "Current iteration=60000, loss=3445.7943375202485\n",
      "Current iteration=61000, loss=3445.767761012536\n",
      "Current iteration=62000, loss=3445.7415673163227\n",
      "Current iteration=63000, loss=3445.7157458865636\n",
      "Current iteration=64000, loss=3445.690286533938\n",
      "Current iteration=65000, loss=3445.6651794112017\n",
      "Current iteration=66000, loss=3445.6404150000953\n",
      "Current iteration=67000, loss=3445.6159840987843\n",
      "Current iteration=68000, loss=3445.5918778098085\n",
      "Current iteration=69000, loss=3445.568087528526\n",
      "Current iteration=70000, loss=3445.544604932027\n",
      "Current iteration=71000, loss=3445.521421968503\n",
      "Current iteration=72000, loss=3445.498530847048\n",
      "Current iteration=73000, loss=3445.475924027878\n",
      "Current iteration=74000, loss=3445.453594212954\n",
      "Current iteration=75000, loss=3445.4315343369826\n",
      "Current iteration=76000, loss=3445.4097375587876\n",
      "Current iteration=77000, loss=3445.388197253033\n",
      "Current iteration=78000, loss=3445.3669070022856\n",
      "Current iteration=79000, loss=3445.345860589393\n",
      "Current iteration=80000, loss=3445.325051990183\n",
      "Current iteration=81000, loss=3445.304475366449\n",
      "Current iteration=82000, loss=3445.2841250592232\n",
      "Current iteration=83000, loss=3445.2639955823233\n",
      "Current iteration=84000, loss=3445.2440816161543\n",
      "Current iteration=85000, loss=3445.2243780017643\n",
      "Current iteration=86000, loss=3445.2048797351354\n",
      "Current iteration=87000, loss=3445.185581961704\n",
      "Current iteration=88000, loss=3445.166479971098\n",
      "Current iteration=89000, loss=3445.1475691920828\n",
      "Current iteration=90000, loss=3445.1288451877153\n",
      "Current iteration=91000, loss=3445.1103036506734\n",
      "Current iteration=92000, loss=3445.0919403987837\n",
      "Current iteration=93000, loss=3445.073751370718\n",
      "Current iteration=94000, loss=3445.055732621859\n",
      "Current iteration=95000, loss=3445.037880320325\n",
      "Current iteration=96000, loss=3445.0201907431524\n",
      "Current iteration=97000, loss=3445.0026602726234\n",
      "Current iteration=98000, loss=3444.985285392735\n",
      "Current iteration=99000, loss=3444.968062685802\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.43615441722345955\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.3323476136593\n",
      "Current iteration=2000, loss=3449.668600217126\n",
      "Current iteration=3000, loss=3449.0293512008448\n",
      "Current iteration=4000, loss=3448.708959281017\n",
      "Current iteration=5000, loss=3448.5153955034943\n",
      "Current iteration=6000, loss=3448.379141200923\n",
      "Current iteration=7000, loss=3448.2711381570034\n",
      "Current iteration=8000, loss=3448.1781421577675\n",
      "Current iteration=9000, loss=3448.0937599467093\n",
      "Current iteration=10000, loss=3448.0147741640076\n",
      "Current iteration=11000, loss=3447.939509817924\n",
      "Current iteration=12000, loss=3447.867066108677\n",
      "Current iteration=13000, loss=3447.796940362527\n",
      "Current iteration=14000, loss=3447.7288384831554\n",
      "Current iteration=15000, loss=3447.662577353737\n",
      "Current iteration=16000, loss=3447.598033785779\n",
      "Current iteration=17000, loss=3447.5351174991224\n",
      "Current iteration=18000, loss=3447.4737566922026\n",
      "Current iteration=19000, loss=3447.4138902811046\n",
      "Current iteration=20000, loss=3447.355463699819\n",
      "Current iteration=21000, loss=3447.298426613645\n",
      "Current iteration=22000, loss=3447.2427316648054\n",
      "Current iteration=23000, loss=3447.188333776584\n",
      "Current iteration=24000, loss=3447.135189760172\n",
      "Current iteration=25000, loss=3447.083258085643\n",
      "Current iteration=26000, loss=3447.032498741839\n",
      "Current iteration=27000, loss=3446.9828731443067\n",
      "Current iteration=28000, loss=3446.934344069119\n",
      "Current iteration=29000, loss=3446.8868756005622\n",
      "Current iteration=30000, loss=3446.8404330862254\n",
      "Current iteration=31000, loss=3446.7949830960583\n",
      "Current iteration=32000, loss=3446.7504933835758\n",
      "Current iteration=33000, loss=3446.706932848302\n",
      "Current iteration=34000, loss=3446.664271499016\n",
      "Current iteration=35000, loss=3446.622480417607\n",
      "Current iteration=36000, loss=3446.581531723498\n",
      "Current iteration=37000, loss=3446.541398538634\n",
      "Current iteration=38000, loss=3446.502054953091\n",
      "Current iteration=39000, loss=3446.463475991336\n",
      "Current iteration=40000, loss=3446.425637579211\n",
      "Current iteration=41000, loss=3446.388516511667\n",
      "Current iteration=42000, loss=3446.352090421294\n",
      "Current iteration=43000, loss=3446.316337747681\n",
      "Current iteration=44000, loss=3446.281237707626\n",
      "Current iteration=45000, loss=3446.246770266216\n",
      "Current iteration=46000, loss=3446.212916108778\n",
      "Current iteration=47000, loss=3446.179656613722\n",
      "Current iteration=48000, loss=3446.1469738262635\n",
      "Current iteration=49000, loss=3446.114850433023\n",
      "Current iteration=50000, loss=3446.083269737503\n",
      "Current iteration=51000, loss=3446.052215636418\n",
      "Current iteration=52000, loss=3446.0216725968753\n",
      "Current iteration=53000, loss=3445.9916256343854\n",
      "Current iteration=54000, loss=3445.962060291687\n",
      "Current iteration=55000, loss=3445.932962618363\n",
      "Current iteration=56000, loss=3445.9043191512305\n",
      "Current iteration=57000, loss=3445.8761168954857\n",
      "Current iteration=58000, loss=3445.8483433065776\n",
      "Current iteration=59000, loss=3445.8209862727863\n",
      "Current iteration=60000, loss=3445.7940340984974\n",
      "Current iteration=61000, loss=3445.767475488129\n",
      "Current iteration=62000, loss=3445.7412995307027\n",
      "Current iteration=63000, loss=3445.7154956850372\n",
      "Current iteration=64000, loss=3445.6900537655315\n",
      "Current iteration=65000, loss=3445.664963928529\n",
      "Current iteration=66000, loss=3445.640216659225\n",
      "Current iteration=67000, loss=3445.61580275912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=68000, loss=3445.5917133339694\n",
      "Current iteration=69000, loss=3445.567939782236\n",
      "Current iteration=70000, loss=3445.5444737840057\n",
      "Current iteration=71000, loss=3445.5213072903625\n",
      "Current iteration=72000, loss=3445.4984325131977\n",
      "Current iteration=73000, loss=3445.47584191543\n",
      "Current iteration=74000, loss=3445.453528201632\n",
      "Current iteration=75000, loss=3445.4314843090365\n",
      "Current iteration=76000, loss=3445.4097033989133\n",
      "Current iteration=77000, loss=3445.3881788482913\n",
      "Current iteration=78000, loss=3445.366904242027\n",
      "Current iteration=79000, loss=3445.3458733651887\n",
      "Current iteration=80000, loss=3445.3250801957524\n",
      "Current iteration=81000, loss=3445.3045188975952\n",
      "Current iteration=82000, loss=3445.284183813768\n",
      "Current iteration=83000, loss=3445.264069460048\n",
      "Current iteration=84000, loss=3445.244170518741\n",
      "Current iteration=85000, loss=3445.224481832738\n",
      "Current iteration=86000, loss=3445.2049983998118\n",
      "Current iteration=87000, loss=3445.1857153671363\n",
      "Current iteration=88000, loss=3445.166628026029\n",
      "Current iteration=89000, loss=3445.1477318068974\n",
      "Current iteration=90000, loss=3445.12902227439\n",
      "Current iteration=91000, loss=3445.110495122738\n",
      "Current iteration=92000, loss=3445.0921461712755\n",
      "Current iteration=93000, loss=3445.0739713601406\n",
      "Current iteration=94000, loss=3445.0559667461453\n",
      "Current iteration=95000, loss=3445.0381284987952\n",
      "Current iteration=96000, loss=3445.020452896482\n",
      "Current iteration=97000, loss=3445.002936322805\n",
      "Current iteration=98000, loss=3444.985575263045\n",
      "Current iteration=99000, loss=3444.9683663007686\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.4361544172234595\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.333543959664\n",
      "Current iteration=2000, loss=3449.670104820748\n",
      "Current iteration=3000, loss=3449.0309818277606\n",
      "Current iteration=4000, loss=3448.7106380060613\n",
      "Current iteration=5000, loss=3448.5170833410757\n",
      "Current iteration=6000, loss=3448.380817283923\n",
      "Current iteration=7000, loss=3448.272791022628\n",
      "Current iteration=8000, loss=3448.1797655212713\n",
      "Current iteration=9000, loss=3448.0953504686217\n",
      "Current iteration=10000, loss=3448.0163302085493\n",
      "Current iteration=11000, loss=3447.9410307409808\n",
      "Current iteration=12000, loss=3447.868551842032\n",
      "Current iteration=13000, loss=3447.798391168664\n",
      "Current iteration=14000, loss=3447.73025481004\n",
      "Current iteration=15000, loss=3447.6639597487774\n",
      "Current iteration=16000, loss=3447.599382844988\n",
      "Current iteration=17000, loss=3447.5364338372783\n",
      "Current iteration=18000, loss=3447.475040925501\n",
      "Current iteration=19000, loss=3447.4151430172537\n",
      "Current iteration=20000, loss=3447.3566855325384\n",
      "Current iteration=21000, loss=3447.2996181197714\n",
      "Current iteration=22000, loss=3447.2438934029283\n",
      "Current iteration=23000, loss=3447.1894662865748\n",
      "Current iteration=24000, loss=3447.13629356323\n",
      "Current iteration=25000, loss=3447.0843336846356\n",
      "Current iteration=26000, loss=3447.033546621803\n",
      "Current iteration=27000, loss=3446.9838937730387\n",
      "Current iteration=28000, loss=3446.935337897801\n",
      "Current iteration=29000, loss=3446.887843064402\n",
      "Current iteration=30000, loss=3446.8413746050996\n",
      "Current iteration=31000, loss=3446.795899075138\n",
      "Current iteration=32000, loss=3446.7513842139397\n",
      "Current iteration=33000, loss=3446.7077989075283\n",
      "Current iteration=34000, loss=3446.665113151754\n",
      "Current iteration=35000, loss=3446.6232980161285\n",
      "Current iteration=36000, loss=3446.582325608225\n",
      "Current iteration=37000, loss=3446.542169038646\n",
      "Current iteration=38000, loss=3446.5028023866103\n",
      "Current iteration=39000, loss=3446.4642006661943\n",
      "Current iteration=40000, loss=3446.4263397932905\n",
      "Current iteration=41000, loss=3446.3891965533276\n",
      "Current iteration=42000, loss=3446.352748569777\n",
      "Current iteration=43000, loss=3446.3169742735\n",
      "Current iteration=44000, loss=3446.2818528729345\n",
      "Current iteration=45000, loss=3446.247364325157\n",
      "Current iteration=46000, loss=3446.213489307825\n",
      "Current iteration=47000, loss=3446.1802091919967\n",
      "Current iteration=48000, loss=3446.147506015842\n",
      "Current iteration=49000, loss=3446.115362459227\n",
      "Current iteration=50000, loss=3446.083761819177\n",
      "Current iteration=51000, loss=3446.0526879861936\n",
      "Current iteration=52000, loss=3446.022125421423\n",
      "Current iteration=53000, loss=3445.992059134656\n",
      "Current iteration=54000, loss=3445.9624746631375\n",
      "Current iteration=55000, loss=3445.9333580511766\n",
      "Current iteration=56000, loss=3445.9046958305225\n",
      "Current iteration=57000, loss=3445.8764750015025\n",
      "Current iteration=58000, loss=3445.848683014884\n",
      "Current iteration=59000, loss=3445.821307754448\n",
      "Current iteration=60000, loss=3445.7943375202485\n",
      "Current iteration=61000, loss=3445.767761012536\n",
      "Current iteration=62000, loss=3445.7415673163227\n",
      "Current iteration=63000, loss=3445.7157458865636\n",
      "Current iteration=64000, loss=3445.690286533938\n",
      "Current iteration=65000, loss=3445.6651794112017\n",
      "Current iteration=66000, loss=3445.6404150000953\n",
      "Current iteration=67000, loss=3445.6159840987843\n",
      "Current iteration=68000, loss=3445.5918778098085\n",
      "Current iteration=69000, loss=3445.568087528526\n",
      "Current iteration=70000, loss=3445.544604932027\n",
      "Current iteration=71000, loss=3445.521421968503\n",
      "Current iteration=72000, loss=3445.498530847048\n",
      "Current iteration=73000, loss=3445.475924027878\n",
      "Current iteration=74000, loss=3445.453594212954\n",
      "Current iteration=75000, loss=3445.4315343369826\n",
      "Current iteration=76000, loss=3445.4097375587876\n",
      "Current iteration=77000, loss=3445.388197253033\n",
      "Current iteration=78000, loss=3445.3669070022856\n",
      "Current iteration=79000, loss=3445.345860589393\n",
      "Current iteration=80000, loss=3445.325051990183\n",
      "Current iteration=81000, loss=3445.304475366449\n",
      "Current iteration=82000, loss=3445.2841250592232\n",
      "Current iteration=83000, loss=3445.2639955823233\n",
      "Current iteration=84000, loss=3445.2440816161543\n",
      "Current iteration=85000, loss=3445.2243780017643\n",
      "Current iteration=86000, loss=3445.2048797351354\n",
      "Current iteration=87000, loss=3445.185581961704\n",
      "Current iteration=88000, loss=3445.166479971098\n",
      "Current iteration=89000, loss=3445.1475691920828\n",
      "Current iteration=90000, loss=3445.1288451877153\n",
      "Current iteration=91000, loss=3445.1103036506734\n",
      "Current iteration=92000, loss=3445.0919403987837\n",
      "Current iteration=93000, loss=3445.073751370718\n",
      "Current iteration=94000, loss=3445.055732621859\n",
      "Current iteration=95000, loss=3445.037880320325\n",
      "Current iteration=96000, loss=3445.0201907431524\n",
      "Current iteration=97000, loss=3445.0026602726234\n",
      "Current iteration=98000, loss=3444.985285392735\n",
      "Current iteration=99000, loss=3444.968062685802\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.43615441722345955\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.3323476136593\n",
      "Current iteration=2000, loss=3449.668600217126\n",
      "Current iteration=3000, loss=3449.0293512008448\n",
      "Current iteration=4000, loss=3448.708959281017\n",
      "Current iteration=5000, loss=3448.5153955034943\n",
      "Current iteration=6000, loss=3448.379141200923\n",
      "Current iteration=7000, loss=3448.2711381570034\n",
      "Current iteration=8000, loss=3448.1781421577675\n",
      "Current iteration=9000, loss=3448.0937599467093\n",
      "Current iteration=10000, loss=3448.0147741640076\n",
      "Current iteration=11000, loss=3447.939509817924\n",
      "Current iteration=12000, loss=3447.867066108677\n",
      "Current iteration=13000, loss=3447.796940362527\n",
      "Current iteration=14000, loss=3447.7288384831554\n",
      "Current iteration=15000, loss=3447.662577353737\n",
      "Current iteration=16000, loss=3447.598033785779\n",
      "Current iteration=17000, loss=3447.5351174991224\n",
      "Current iteration=18000, loss=3447.4737566922026\n",
      "Current iteration=19000, loss=3447.4138902811046\n",
      "Current iteration=20000, loss=3447.355463699819\n",
      "Current iteration=21000, loss=3447.298426613645\n",
      "Current iteration=22000, loss=3447.2427316648054\n",
      "Current iteration=23000, loss=3447.188333776584\n",
      "Current iteration=24000, loss=3447.135189760172\n",
      "Current iteration=25000, loss=3447.083258085643\n",
      "Current iteration=26000, loss=3447.032498741839\n",
      "Current iteration=27000, loss=3446.9828731443067\n",
      "Current iteration=28000, loss=3446.934344069119\n",
      "Current iteration=29000, loss=3446.8868756005622\n",
      "Current iteration=30000, loss=3446.8404330862254\n",
      "Current iteration=31000, loss=3446.7949830960583\n",
      "Current iteration=32000, loss=3446.7504933835758\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=33000, loss=3446.706932848302\n",
      "Current iteration=34000, loss=3446.664271499016\n",
      "Current iteration=35000, loss=3446.622480417607\n",
      "Current iteration=36000, loss=3446.581531723498\n",
      "Current iteration=37000, loss=3446.541398538634\n",
      "Current iteration=38000, loss=3446.502054953091\n",
      "Current iteration=39000, loss=3446.463475991336\n",
      "Current iteration=40000, loss=3446.425637579211\n",
      "Current iteration=41000, loss=3446.388516511667\n",
      "Current iteration=42000, loss=3446.352090421294\n",
      "Current iteration=43000, loss=3446.316337747681\n",
      "Current iteration=44000, loss=3446.281237707626\n",
      "Current iteration=45000, loss=3446.246770266216\n",
      "Current iteration=46000, loss=3446.212916108778\n",
      "Current iteration=47000, loss=3446.179656613722\n",
      "Current iteration=48000, loss=3446.1469738262635\n",
      "Current iteration=49000, loss=3446.114850433023\n",
      "Current iteration=50000, loss=3446.083269737503\n",
      "Current iteration=51000, loss=3446.052215636418\n",
      "Current iteration=52000, loss=3446.0216725968753\n",
      "Current iteration=53000, loss=3445.9916256343854\n",
      "Current iteration=54000, loss=3445.962060291687\n",
      "Current iteration=55000, loss=3445.932962618363\n",
      "Current iteration=56000, loss=3445.9043191512305\n",
      "Current iteration=57000, loss=3445.8761168954857\n",
      "Current iteration=58000, loss=3445.8483433065776\n",
      "Current iteration=59000, loss=3445.8209862727863\n",
      "Current iteration=60000, loss=3445.7940340984974\n",
      "Current iteration=61000, loss=3445.767475488129\n",
      "Current iteration=62000, loss=3445.7412995307027\n",
      "Current iteration=63000, loss=3445.7154956850372\n",
      "Current iteration=64000, loss=3445.6900537655315\n",
      "Current iteration=65000, loss=3445.664963928529\n",
      "Current iteration=66000, loss=3445.640216659225\n",
      "Current iteration=67000, loss=3445.61580275912\n",
      "Current iteration=68000, loss=3445.5917133339694\n",
      "Current iteration=69000, loss=3445.567939782236\n",
      "Current iteration=70000, loss=3445.5444737840057\n",
      "Current iteration=71000, loss=3445.5213072903625\n",
      "Current iteration=72000, loss=3445.4984325131977\n",
      "Current iteration=73000, loss=3445.47584191543\n",
      "Current iteration=74000, loss=3445.453528201632\n",
      "Current iteration=75000, loss=3445.4314843090365\n",
      "Current iteration=76000, loss=3445.4097033989133\n",
      "Current iteration=77000, loss=3445.3881788482913\n",
      "Current iteration=78000, loss=3445.366904242027\n",
      "Current iteration=79000, loss=3445.3458733651887\n",
      "Current iteration=80000, loss=3445.3250801957524\n",
      "Current iteration=81000, loss=3445.3045188975952\n",
      "Current iteration=82000, loss=3445.284183813768\n",
      "Current iteration=83000, loss=3445.264069460048\n",
      "Current iteration=84000, loss=3445.244170518741\n",
      "Current iteration=85000, loss=3445.224481832738\n",
      "Current iteration=86000, loss=3445.2049983998118\n",
      "Current iteration=87000, loss=3445.1857153671363\n",
      "Current iteration=88000, loss=3445.166628026029\n",
      "Current iteration=89000, loss=3445.1477318068974\n",
      "Current iteration=90000, loss=3445.12902227439\n",
      "Current iteration=91000, loss=3445.110495122738\n",
      "Current iteration=92000, loss=3445.0921461712755\n",
      "Current iteration=93000, loss=3445.0739713601406\n",
      "Current iteration=94000, loss=3445.0559667461453\n",
      "Current iteration=95000, loss=3445.0381284987952\n",
      "Current iteration=96000, loss=3445.020452896482\n",
      "Current iteration=97000, loss=3445.002936322805\n",
      "Current iteration=98000, loss=3444.985575263045\n",
      "Current iteration=99000, loss=3444.9683663007686\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.4361544172234595\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.333543959664\n",
      "Current iteration=2000, loss=3449.670104820748\n",
      "Current iteration=3000, loss=3449.0309818277606\n",
      "Current iteration=4000, loss=3448.7106380060613\n",
      "Current iteration=5000, loss=3448.5170833410757\n",
      "Current iteration=6000, loss=3448.380817283923\n",
      "Current iteration=7000, loss=3448.272791022628\n",
      "Current iteration=8000, loss=3448.1797655212713\n",
      "Current iteration=9000, loss=3448.0953504686217\n",
      "Current iteration=10000, loss=3448.0163302085493\n",
      "Current iteration=11000, loss=3447.9410307409808\n",
      "Current iteration=12000, loss=3447.868551842032\n",
      "Current iteration=13000, loss=3447.798391168664\n",
      "Current iteration=14000, loss=3447.73025481004\n",
      "Current iteration=15000, loss=3447.6639597487774\n",
      "Current iteration=16000, loss=3447.599382844988\n",
      "Current iteration=17000, loss=3447.5364338372783\n",
      "Current iteration=18000, loss=3447.475040925501\n",
      "Current iteration=19000, loss=3447.4151430172537\n",
      "Current iteration=20000, loss=3447.3566855325384\n",
      "Current iteration=21000, loss=3447.2996181197714\n",
      "Current iteration=22000, loss=3447.2438934029283\n",
      "Current iteration=23000, loss=3447.1894662865748\n",
      "Current iteration=24000, loss=3447.13629356323\n",
      "Current iteration=25000, loss=3447.0843336846356\n",
      "Current iteration=26000, loss=3447.033546621803\n",
      "Current iteration=27000, loss=3446.9838937730387\n",
      "Current iteration=28000, loss=3446.935337897801\n",
      "Current iteration=29000, loss=3446.887843064402\n",
      "Current iteration=30000, loss=3446.8413746050996\n",
      "Current iteration=31000, loss=3446.795899075138\n",
      "Current iteration=32000, loss=3446.7513842139397\n",
      "Current iteration=33000, loss=3446.7077989075283\n",
      "Current iteration=34000, loss=3446.665113151754\n",
      "Current iteration=35000, loss=3446.6232980161285\n",
      "Current iteration=36000, loss=3446.582325608225\n",
      "Current iteration=37000, loss=3446.542169038646\n",
      "Current iteration=38000, loss=3446.5028023866103\n",
      "Current iteration=39000, loss=3446.4642006661943\n",
      "Current iteration=40000, loss=3446.4263397932905\n",
      "Current iteration=41000, loss=3446.3891965533276\n",
      "Current iteration=42000, loss=3446.352748569777\n",
      "Current iteration=43000, loss=3446.3169742735\n",
      "Current iteration=44000, loss=3446.2818528729345\n",
      "Current iteration=45000, loss=3446.247364325157\n",
      "Current iteration=46000, loss=3446.213489307825\n",
      "Current iteration=47000, loss=3446.1802091919967\n",
      "Current iteration=48000, loss=3446.147506015842\n",
      "Current iteration=49000, loss=3446.115362459227\n",
      "Current iteration=50000, loss=3446.083761819177\n",
      "Current iteration=51000, loss=3446.0526879861936\n",
      "Current iteration=52000, loss=3446.022125421423\n",
      "Current iteration=53000, loss=3445.992059134656\n",
      "Current iteration=54000, loss=3445.9624746631375\n",
      "Current iteration=55000, loss=3445.9333580511766\n",
      "Current iteration=56000, loss=3445.9046958305225\n",
      "Current iteration=57000, loss=3445.8764750015025\n",
      "Current iteration=58000, loss=3445.848683014884\n",
      "Current iteration=59000, loss=3445.821307754448\n",
      "Current iteration=60000, loss=3445.7943375202485\n",
      "Current iteration=61000, loss=3445.767761012536\n",
      "Current iteration=62000, loss=3445.7415673163227\n",
      "Current iteration=63000, loss=3445.7157458865636\n",
      "Current iteration=64000, loss=3445.690286533938\n",
      "Current iteration=65000, loss=3445.6651794112017\n",
      "Current iteration=66000, loss=3445.6404150000953\n",
      "Current iteration=67000, loss=3445.6159840987843\n",
      "Current iteration=68000, loss=3445.5918778098085\n",
      "Current iteration=69000, loss=3445.568087528526\n",
      "Current iteration=70000, loss=3445.544604932027\n",
      "Current iteration=71000, loss=3445.521421968503\n",
      "Current iteration=72000, loss=3445.498530847048\n",
      "Current iteration=73000, loss=3445.475924027878\n",
      "Current iteration=74000, loss=3445.453594212954\n",
      "Current iteration=75000, loss=3445.4315343369826\n",
      "Current iteration=76000, loss=3445.4097375587876\n",
      "Current iteration=77000, loss=3445.388197253033\n",
      "Current iteration=78000, loss=3445.3669070022856\n",
      "Current iteration=79000, loss=3445.345860589393\n",
      "Current iteration=80000, loss=3445.325051990183\n",
      "Current iteration=81000, loss=3445.304475366449\n",
      "Current iteration=82000, loss=3445.2841250592232\n",
      "Current iteration=83000, loss=3445.2639955823233\n",
      "Current iteration=84000, loss=3445.2440816161543\n",
      "Current iteration=85000, loss=3445.2243780017643\n",
      "Current iteration=86000, loss=3445.2048797351354\n",
      "Current iteration=87000, loss=3445.185581961704\n",
      "Current iteration=88000, loss=3445.166479971098\n",
      "Current iteration=89000, loss=3445.1475691920828\n",
      "Current iteration=90000, loss=3445.1288451877153\n",
      "Current iteration=91000, loss=3445.1103036506734\n",
      "Current iteration=92000, loss=3445.0919403987837\n",
      "Current iteration=93000, loss=3445.073751370718\n",
      "Current iteration=94000, loss=3445.055732621859\n",
      "Current iteration=95000, loss=3445.037880320325\n",
      "Current iteration=96000, loss=3445.0201907431524\n",
      "Current iteration=97000, loss=3445.0026602726234\n",
      "Current iteration=98000, loss=3444.985285392735\n",
      "Current iteration=99000, loss=3444.968062685802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.43615441722345955\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.3323476136593\n",
      "Current iteration=2000, loss=3449.668600217126\n",
      "Current iteration=3000, loss=3449.0293512008448\n",
      "Current iteration=4000, loss=3448.708959281017\n",
      "Current iteration=5000, loss=3448.5153955034943\n",
      "Current iteration=6000, loss=3448.379141200923\n",
      "Current iteration=7000, loss=3448.2711381570034\n",
      "Current iteration=8000, loss=3448.1781421577675\n",
      "Current iteration=9000, loss=3448.0937599467093\n",
      "Current iteration=10000, loss=3448.0147741640076\n",
      "Current iteration=11000, loss=3447.939509817924\n",
      "Current iteration=12000, loss=3447.867066108677\n",
      "Current iteration=13000, loss=3447.796940362527\n",
      "Current iteration=14000, loss=3447.7288384831554\n",
      "Current iteration=15000, loss=3447.662577353737\n",
      "Current iteration=16000, loss=3447.598033785779\n",
      "Current iteration=17000, loss=3447.5351174991224\n",
      "Current iteration=18000, loss=3447.4737566922026\n",
      "Current iteration=19000, loss=3447.4138902811046\n",
      "Current iteration=20000, loss=3447.355463699819\n",
      "Current iteration=21000, loss=3447.298426613645\n",
      "Current iteration=22000, loss=3447.2427316648054\n",
      "Current iteration=23000, loss=3447.188333776584\n",
      "Current iteration=24000, loss=3447.135189760172\n",
      "Current iteration=25000, loss=3447.083258085643\n",
      "Current iteration=26000, loss=3447.032498741839\n",
      "Current iteration=27000, loss=3446.9828731443067\n",
      "Current iteration=28000, loss=3446.934344069119\n",
      "Current iteration=29000, loss=3446.8868756005622\n",
      "Current iteration=30000, loss=3446.8404330862254\n",
      "Current iteration=31000, loss=3446.7949830960583\n",
      "Current iteration=32000, loss=3446.7504933835758\n",
      "Current iteration=33000, loss=3446.706932848302\n",
      "Current iteration=34000, loss=3446.664271499016\n",
      "Current iteration=35000, loss=3446.622480417607\n",
      "Current iteration=36000, loss=3446.581531723498\n",
      "Current iteration=37000, loss=3446.541398538634\n",
      "Current iteration=38000, loss=3446.502054953091\n",
      "Current iteration=39000, loss=3446.463475991336\n",
      "Current iteration=40000, loss=3446.425637579211\n",
      "Current iteration=41000, loss=3446.388516511667\n",
      "Current iteration=42000, loss=3446.352090421294\n",
      "Current iteration=43000, loss=3446.316337747681\n",
      "Current iteration=44000, loss=3446.281237707626\n",
      "Current iteration=45000, loss=3446.246770266216\n",
      "Current iteration=46000, loss=3446.212916108778\n",
      "Current iteration=47000, loss=3446.179656613722\n",
      "Current iteration=48000, loss=3446.1469738262635\n",
      "Current iteration=49000, loss=3446.114850433023\n",
      "Current iteration=50000, loss=3446.083269737503\n",
      "Current iteration=51000, loss=3446.052215636418\n",
      "Current iteration=52000, loss=3446.0216725968753\n",
      "Current iteration=53000, loss=3445.9916256343854\n",
      "Current iteration=54000, loss=3445.962060291687\n",
      "Current iteration=55000, loss=3445.932962618363\n",
      "Current iteration=56000, loss=3445.9043191512305\n",
      "Current iteration=57000, loss=3445.8761168954857\n",
      "Current iteration=58000, loss=3445.8483433065776\n",
      "Current iteration=59000, loss=3445.8209862727863\n",
      "Current iteration=60000, loss=3445.7940340984974\n",
      "Current iteration=61000, loss=3445.767475488129\n",
      "Current iteration=62000, loss=3445.7412995307027\n",
      "Current iteration=63000, loss=3445.7154956850372\n",
      "Current iteration=64000, loss=3445.6900537655315\n",
      "Current iteration=65000, loss=3445.664963928529\n",
      "Current iteration=66000, loss=3445.640216659225\n",
      "Current iteration=67000, loss=3445.61580275912\n",
      "Current iteration=68000, loss=3445.5917133339694\n",
      "Current iteration=69000, loss=3445.567939782236\n",
      "Current iteration=70000, loss=3445.5444737840057\n",
      "Current iteration=71000, loss=3445.5213072903625\n",
      "Current iteration=72000, loss=3445.4984325131977\n",
      "Current iteration=73000, loss=3445.47584191543\n",
      "Current iteration=74000, loss=3445.453528201632\n",
      "Current iteration=75000, loss=3445.4314843090365\n",
      "Current iteration=76000, loss=3445.4097033989133\n",
      "Current iteration=77000, loss=3445.3881788482913\n",
      "Current iteration=78000, loss=3445.366904242027\n",
      "Current iteration=79000, loss=3445.3458733651887\n",
      "Current iteration=80000, loss=3445.3250801957524\n",
      "Current iteration=81000, loss=3445.3045188975952\n",
      "Current iteration=82000, loss=3445.284183813768\n",
      "Current iteration=83000, loss=3445.264069460048\n",
      "Current iteration=84000, loss=3445.244170518741\n",
      "Current iteration=85000, loss=3445.224481832738\n",
      "Current iteration=86000, loss=3445.2049983998118\n",
      "Current iteration=87000, loss=3445.1857153671363\n",
      "Current iteration=88000, loss=3445.166628026029\n",
      "Current iteration=89000, loss=3445.1477318068974\n",
      "Current iteration=90000, loss=3445.12902227439\n",
      "Current iteration=91000, loss=3445.110495122738\n",
      "Current iteration=92000, loss=3445.0921461712755\n",
      "Current iteration=93000, loss=3445.0739713601406\n",
      "Current iteration=94000, loss=3445.0559667461453\n",
      "Current iteration=95000, loss=3445.0381284987952\n",
      "Current iteration=96000, loss=3445.020452896482\n",
      "Current iteration=97000, loss=3445.002936322805\n",
      "Current iteration=98000, loss=3444.985575263045\n",
      "Current iteration=99000, loss=3444.9683663007686\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.4361544172234595\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.333543959664\n",
      "Current iteration=2000, loss=3449.670104820748\n",
      "Current iteration=3000, loss=3449.0309818277606\n",
      "Current iteration=4000, loss=3448.7106380060613\n",
      "Current iteration=5000, loss=3448.5170833410757\n",
      "Current iteration=6000, loss=3448.380817283923\n",
      "Current iteration=7000, loss=3448.272791022628\n",
      "Current iteration=8000, loss=3448.1797655212713\n",
      "Current iteration=9000, loss=3448.0953504686217\n",
      "Current iteration=10000, loss=3448.0163302085493\n",
      "Current iteration=11000, loss=3447.9410307409808\n",
      "Current iteration=12000, loss=3447.868551842032\n",
      "Current iteration=13000, loss=3447.798391168664\n",
      "Current iteration=14000, loss=3447.73025481004\n",
      "Current iteration=15000, loss=3447.6639597487774\n",
      "Current iteration=16000, loss=3447.599382844988\n",
      "Current iteration=17000, loss=3447.5364338372783\n",
      "Current iteration=18000, loss=3447.475040925501\n",
      "Current iteration=19000, loss=3447.4151430172537\n",
      "Current iteration=20000, loss=3447.3566855325384\n",
      "Current iteration=21000, loss=3447.2996181197714\n",
      "Current iteration=22000, loss=3447.2438934029283\n",
      "Current iteration=23000, loss=3447.1894662865748\n",
      "Current iteration=24000, loss=3447.13629356323\n",
      "Current iteration=25000, loss=3447.0843336846356\n",
      "Current iteration=26000, loss=3447.033546621803\n",
      "Current iteration=27000, loss=3446.9838937730387\n",
      "Current iteration=28000, loss=3446.935337897801\n",
      "Current iteration=29000, loss=3446.887843064402\n",
      "Current iteration=30000, loss=3446.8413746050996\n",
      "Current iteration=31000, loss=3446.795899075138\n",
      "Current iteration=32000, loss=3446.7513842139397\n",
      "Current iteration=33000, loss=3446.7077989075283\n",
      "Current iteration=34000, loss=3446.665113151754\n",
      "Current iteration=35000, loss=3446.6232980161285\n",
      "Current iteration=36000, loss=3446.582325608225\n",
      "Current iteration=37000, loss=3446.542169038646\n",
      "Current iteration=38000, loss=3446.5028023866103\n",
      "Current iteration=39000, loss=3446.4642006661943\n",
      "Current iteration=40000, loss=3446.4263397932905\n",
      "Current iteration=41000, loss=3446.3891965533276\n",
      "Current iteration=42000, loss=3446.352748569777\n",
      "Current iteration=43000, loss=3446.3169742735\n",
      "Current iteration=44000, loss=3446.2818528729345\n",
      "Current iteration=45000, loss=3446.247364325157\n",
      "Current iteration=46000, loss=3446.213489307825\n",
      "Current iteration=47000, loss=3446.1802091919967\n",
      "Current iteration=48000, loss=3446.147506015842\n",
      "Current iteration=49000, loss=3446.115362459227\n",
      "Current iteration=50000, loss=3446.083761819177\n",
      "Current iteration=51000, loss=3446.0526879861936\n",
      "Current iteration=52000, loss=3446.022125421423\n",
      "Current iteration=53000, loss=3445.992059134656\n",
      "Current iteration=54000, loss=3445.9624746631375\n",
      "Current iteration=55000, loss=3445.9333580511766\n",
      "Current iteration=56000, loss=3445.9046958305225\n",
      "Current iteration=57000, loss=3445.8764750015025\n",
      "Current iteration=58000, loss=3445.848683014884\n",
      "Current iteration=59000, loss=3445.821307754448\n",
      "Current iteration=60000, loss=3445.7943375202485\n",
      "Current iteration=61000, loss=3445.767761012536\n",
      "Current iteration=62000, loss=3445.7415673163227\n",
      "Current iteration=63000, loss=3445.7157458865636\n",
      "Current iteration=64000, loss=3445.690286533938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=65000, loss=3445.6651794112017\n",
      "Current iteration=66000, loss=3445.6404150000953\n",
      "Current iteration=67000, loss=3445.6159840987843\n",
      "Current iteration=68000, loss=3445.5918778098085\n",
      "Current iteration=69000, loss=3445.568087528526\n",
      "Current iteration=70000, loss=3445.544604932027\n",
      "Current iteration=71000, loss=3445.521421968503\n",
      "Current iteration=72000, loss=3445.498530847048\n",
      "Current iteration=73000, loss=3445.475924027878\n",
      "Current iteration=74000, loss=3445.453594212954\n",
      "Current iteration=75000, loss=3445.4315343369826\n",
      "Current iteration=76000, loss=3445.4097375587876\n",
      "Current iteration=77000, loss=3445.388197253033\n",
      "Current iteration=78000, loss=3445.3669070022856\n",
      "Current iteration=79000, loss=3445.345860589393\n",
      "Current iteration=80000, loss=3445.325051990183\n",
      "Current iteration=81000, loss=3445.304475366449\n",
      "Current iteration=82000, loss=3445.2841250592232\n",
      "Current iteration=83000, loss=3445.2639955823233\n",
      "Current iteration=84000, loss=3445.2440816161543\n",
      "Current iteration=85000, loss=3445.2243780017643\n",
      "Current iteration=86000, loss=3445.2048797351354\n",
      "Current iteration=87000, loss=3445.185581961704\n",
      "Current iteration=88000, loss=3445.166479971098\n",
      "Current iteration=89000, loss=3445.1475691920828\n",
      "Current iteration=90000, loss=3445.1288451877153\n",
      "Current iteration=91000, loss=3445.1103036506734\n",
      "Current iteration=92000, loss=3445.0919403987837\n",
      "Current iteration=93000, loss=3445.073751370718\n",
      "Current iteration=94000, loss=3445.055732621859\n",
      "Current iteration=95000, loss=3445.037880320325\n",
      "Current iteration=96000, loss=3445.0201907431524\n",
      "Current iteration=97000, loss=3445.0026602726234\n",
      "Current iteration=98000, loss=3444.985285392735\n",
      "Current iteration=99000, loss=3444.968062685802\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.43615441722345955\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.3323476136593\n",
      "Current iteration=2000, loss=3449.668600217126\n",
      "Current iteration=3000, loss=3449.0293512008448\n",
      "Current iteration=4000, loss=3448.708959281017\n",
      "Current iteration=5000, loss=3448.5153955034943\n",
      "Current iteration=6000, loss=3448.379141200923\n",
      "Current iteration=7000, loss=3448.2711381570034\n",
      "Current iteration=8000, loss=3448.1781421577675\n",
      "Current iteration=9000, loss=3448.0937599467093\n",
      "Current iteration=10000, loss=3448.0147741640076\n",
      "Current iteration=11000, loss=3447.939509817924\n",
      "Current iteration=12000, loss=3447.867066108677\n",
      "Current iteration=13000, loss=3447.796940362527\n",
      "Current iteration=14000, loss=3447.7288384831554\n",
      "Current iteration=15000, loss=3447.662577353737\n",
      "Current iteration=16000, loss=3447.598033785779\n",
      "Current iteration=17000, loss=3447.5351174991224\n",
      "Current iteration=18000, loss=3447.4737566922026\n",
      "Current iteration=19000, loss=3447.4138902811046\n",
      "Current iteration=20000, loss=3447.355463699819\n",
      "Current iteration=21000, loss=3447.298426613645\n",
      "Current iteration=22000, loss=3447.2427316648054\n",
      "Current iteration=23000, loss=3447.188333776584\n",
      "Current iteration=24000, loss=3447.135189760172\n",
      "Current iteration=25000, loss=3447.083258085643\n",
      "Current iteration=26000, loss=3447.032498741839\n",
      "Current iteration=27000, loss=3446.9828731443067\n",
      "Current iteration=28000, loss=3446.934344069119\n",
      "Current iteration=29000, loss=3446.8868756005622\n",
      "Current iteration=30000, loss=3446.8404330862254\n",
      "Current iteration=31000, loss=3446.7949830960583\n",
      "Current iteration=32000, loss=3446.7504933835758\n",
      "Current iteration=33000, loss=3446.706932848302\n",
      "Current iteration=34000, loss=3446.664271499016\n",
      "Current iteration=35000, loss=3446.622480417607\n",
      "Current iteration=36000, loss=3446.581531723498\n",
      "Current iteration=37000, loss=3446.541398538634\n",
      "Current iteration=38000, loss=3446.502054953091\n",
      "Current iteration=39000, loss=3446.463475991336\n",
      "Current iteration=40000, loss=3446.425637579211\n",
      "Current iteration=41000, loss=3446.388516511667\n",
      "Current iteration=42000, loss=3446.352090421294\n",
      "Current iteration=43000, loss=3446.316337747681\n",
      "Current iteration=44000, loss=3446.281237707626\n",
      "Current iteration=45000, loss=3446.246770266216\n",
      "Current iteration=46000, loss=3446.212916108778\n",
      "Current iteration=47000, loss=3446.179656613722\n",
      "Current iteration=48000, loss=3446.1469738262635\n",
      "Current iteration=49000, loss=3446.114850433023\n",
      "Current iteration=50000, loss=3446.083269737503\n",
      "Current iteration=51000, loss=3446.052215636418\n",
      "Current iteration=52000, loss=3446.0216725968753\n",
      "Current iteration=53000, loss=3445.9916256343854\n",
      "Current iteration=54000, loss=3445.962060291687\n",
      "Current iteration=55000, loss=3445.932962618363\n",
      "Current iteration=56000, loss=3445.9043191512305\n",
      "Current iteration=57000, loss=3445.8761168954857\n",
      "Current iteration=58000, loss=3445.8483433065776\n",
      "Current iteration=59000, loss=3445.8209862727863\n",
      "Current iteration=60000, loss=3445.7940340984974\n",
      "Current iteration=61000, loss=3445.767475488129\n",
      "Current iteration=62000, loss=3445.7412995307027\n",
      "Current iteration=63000, loss=3445.7154956850372\n",
      "Current iteration=64000, loss=3445.6900537655315\n",
      "Current iteration=65000, loss=3445.664963928529\n",
      "Current iteration=66000, loss=3445.640216659225\n",
      "Current iteration=67000, loss=3445.61580275912\n",
      "Current iteration=68000, loss=3445.5917133339694\n",
      "Current iteration=69000, loss=3445.567939782236\n",
      "Current iteration=70000, loss=3445.5444737840057\n",
      "Current iteration=71000, loss=3445.5213072903625\n",
      "Current iteration=72000, loss=3445.4984325131977\n",
      "Current iteration=73000, loss=3445.47584191543\n",
      "Current iteration=74000, loss=3445.453528201632\n",
      "Current iteration=75000, loss=3445.4314843090365\n",
      "Current iteration=76000, loss=3445.4097033989133\n",
      "Current iteration=77000, loss=3445.3881788482913\n",
      "Current iteration=78000, loss=3445.366904242027\n",
      "Current iteration=79000, loss=3445.3458733651887\n",
      "Current iteration=80000, loss=3445.3250801957524\n",
      "Current iteration=81000, loss=3445.3045188975952\n",
      "Current iteration=82000, loss=3445.284183813768\n",
      "Current iteration=83000, loss=3445.264069460048\n",
      "Current iteration=84000, loss=3445.244170518741\n",
      "Current iteration=85000, loss=3445.224481832738\n",
      "Current iteration=86000, loss=3445.2049983998118\n",
      "Current iteration=87000, loss=3445.1857153671363\n",
      "Current iteration=88000, loss=3445.166628026029\n",
      "Current iteration=89000, loss=3445.1477318068974\n",
      "Current iteration=90000, loss=3445.12902227439\n",
      "Current iteration=91000, loss=3445.110495122738\n",
      "Current iteration=92000, loss=3445.0921461712755\n",
      "Current iteration=93000, loss=3445.0739713601406\n",
      "Current iteration=94000, loss=3445.0559667461453\n",
      "Current iteration=95000, loss=3445.0381284987952\n",
      "Current iteration=96000, loss=3445.020452896482\n",
      "Current iteration=97000, loss=3445.002936322805\n",
      "Current iteration=98000, loss=3444.985575263045\n",
      "Current iteration=99000, loss=3444.9683663007686\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.4361544172234595\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.333543959664\n",
      "Current iteration=2000, loss=3449.670104820748\n",
      "Current iteration=3000, loss=3449.0309818277606\n",
      "Current iteration=4000, loss=3448.7106380060613\n",
      "Current iteration=5000, loss=3448.5170833410757\n",
      "Current iteration=6000, loss=3448.380817283923\n",
      "Current iteration=7000, loss=3448.272791022628\n",
      "Current iteration=8000, loss=3448.1797655212713\n",
      "Current iteration=9000, loss=3448.0953504686217\n",
      "Current iteration=10000, loss=3448.0163302085493\n",
      "Current iteration=11000, loss=3447.9410307409808\n",
      "Current iteration=12000, loss=3447.868551842032\n",
      "Current iteration=13000, loss=3447.798391168664\n",
      "Current iteration=14000, loss=3447.73025481004\n",
      "Current iteration=15000, loss=3447.6639597487774\n",
      "Current iteration=16000, loss=3447.599382844988\n",
      "Current iteration=17000, loss=3447.5364338372783\n",
      "Current iteration=18000, loss=3447.475040925501\n",
      "Current iteration=19000, loss=3447.4151430172537\n",
      "Current iteration=20000, loss=3447.3566855325384\n",
      "Current iteration=21000, loss=3447.2996181197714\n",
      "Current iteration=22000, loss=3447.2438934029283\n",
      "Current iteration=23000, loss=3447.1894662865748\n",
      "Current iteration=24000, loss=3447.13629356323\n",
      "Current iteration=25000, loss=3447.0843336846356\n",
      "Current iteration=26000, loss=3447.033546621803\n",
      "Current iteration=27000, loss=3446.9838937730387\n",
      "Current iteration=28000, loss=3446.935337897801\n",
      "Current iteration=29000, loss=3446.887843064402\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=30000, loss=3446.8413746050996\n",
      "Current iteration=31000, loss=3446.795899075138\n",
      "Current iteration=32000, loss=3446.7513842139397\n",
      "Current iteration=33000, loss=3446.7077989075283\n",
      "Current iteration=34000, loss=3446.665113151754\n",
      "Current iteration=35000, loss=3446.6232980161285\n",
      "Current iteration=36000, loss=3446.582325608225\n",
      "Current iteration=37000, loss=3446.542169038646\n",
      "Current iteration=38000, loss=3446.5028023866103\n",
      "Current iteration=39000, loss=3446.4642006661943\n",
      "Current iteration=40000, loss=3446.4263397932905\n",
      "Current iteration=41000, loss=3446.3891965533276\n",
      "Current iteration=42000, loss=3446.352748569777\n",
      "Current iteration=43000, loss=3446.3169742735\n",
      "Current iteration=44000, loss=3446.2818528729345\n",
      "Current iteration=45000, loss=3446.247364325157\n",
      "Current iteration=46000, loss=3446.213489307825\n",
      "Current iteration=47000, loss=3446.1802091919967\n",
      "Current iteration=48000, loss=3446.147506015842\n",
      "Current iteration=49000, loss=3446.115362459227\n",
      "Current iteration=50000, loss=3446.083761819177\n",
      "Current iteration=51000, loss=3446.0526879861936\n",
      "Current iteration=52000, loss=3446.022125421423\n",
      "Current iteration=53000, loss=3445.992059134656\n",
      "Current iteration=54000, loss=3445.9624746631375\n",
      "Current iteration=55000, loss=3445.9333580511766\n",
      "Current iteration=56000, loss=3445.9046958305225\n",
      "Current iteration=57000, loss=3445.8764750015025\n",
      "Current iteration=58000, loss=3445.848683014884\n",
      "Current iteration=59000, loss=3445.821307754448\n",
      "Current iteration=60000, loss=3445.7943375202485\n",
      "Current iteration=61000, loss=3445.767761012536\n",
      "Current iteration=62000, loss=3445.7415673163227\n",
      "Current iteration=63000, loss=3445.7157458865636\n",
      "Current iteration=64000, loss=3445.690286533938\n",
      "Current iteration=65000, loss=3445.6651794112017\n",
      "Current iteration=66000, loss=3445.6404150000953\n",
      "Current iteration=67000, loss=3445.6159840987843\n",
      "Current iteration=68000, loss=3445.5918778098085\n",
      "Current iteration=69000, loss=3445.568087528526\n",
      "Current iteration=70000, loss=3445.544604932027\n",
      "Current iteration=71000, loss=3445.521421968503\n",
      "Current iteration=72000, loss=3445.498530847048\n",
      "Current iteration=73000, loss=3445.475924027878\n",
      "Current iteration=74000, loss=3445.453594212954\n",
      "Current iteration=75000, loss=3445.4315343369826\n",
      "Current iteration=76000, loss=3445.4097375587876\n",
      "Current iteration=77000, loss=3445.388197253033\n",
      "Current iteration=78000, loss=3445.3669070022856\n",
      "Current iteration=79000, loss=3445.345860589393\n",
      "Current iteration=80000, loss=3445.325051990183\n",
      "Current iteration=81000, loss=3445.304475366449\n",
      "Current iteration=82000, loss=3445.2841250592232\n",
      "Current iteration=83000, loss=3445.2639955823233\n",
      "Current iteration=84000, loss=3445.2440816161543\n",
      "Current iteration=85000, loss=3445.2243780017643\n",
      "Current iteration=86000, loss=3445.2048797351354\n",
      "Current iteration=87000, loss=3445.185581961704\n",
      "Current iteration=88000, loss=3445.166479971098\n",
      "Current iteration=89000, loss=3445.1475691920828\n",
      "Current iteration=90000, loss=3445.1288451877153\n",
      "Current iteration=91000, loss=3445.1103036506734\n",
      "Current iteration=92000, loss=3445.0919403987837\n",
      "Current iteration=93000, loss=3445.073751370718\n",
      "Current iteration=94000, loss=3445.055732621859\n",
      "Current iteration=95000, loss=3445.037880320325\n",
      "Current iteration=96000, loss=3445.0201907431524\n",
      "Current iteration=97000, loss=3445.0026602726234\n",
      "Current iteration=98000, loss=3444.985285392735\n",
      "Current iteration=99000, loss=3444.968062685802\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.43615441722345955\n",
      "Current iteration=0, loss=3465.042755619167\n",
      "Current iteration=1000, loss=3451.3323476136593\n",
      "Current iteration=2000, loss=3449.668600217126\n",
      "Current iteration=3000, loss=3449.0293512008448\n",
      "Current iteration=4000, loss=3448.708959281017\n",
      "Current iteration=5000, loss=3448.5153955034943\n",
      "Current iteration=6000, loss=3448.379141200923\n",
      "Current iteration=7000, loss=3448.2711381570034\n",
      "Current iteration=8000, loss=3448.1781421577675\n",
      "Current iteration=9000, loss=3448.0937599467093\n",
      "Current iteration=10000, loss=3448.0147741640076\n",
      "Current iteration=11000, loss=3447.939509817924\n",
      "Current iteration=12000, loss=3447.867066108677\n",
      "Current iteration=13000, loss=3447.796940362527\n",
      "Current iteration=14000, loss=3447.7288384831554\n",
      "Current iteration=15000, loss=3447.662577353737\n",
      "Current iteration=16000, loss=3447.598033785779\n",
      "Current iteration=17000, loss=3447.5351174991224\n",
      "Current iteration=18000, loss=3447.4737566922026\n",
      "Current iteration=19000, loss=3447.4138902811046\n",
      "Current iteration=20000, loss=3447.355463699819\n",
      "Current iteration=21000, loss=3447.298426613645\n",
      "Current iteration=22000, loss=3447.2427316648054\n",
      "Current iteration=23000, loss=3447.188333776584\n",
      "Current iteration=24000, loss=3447.135189760172\n",
      "Current iteration=25000, loss=3447.083258085643\n",
      "Current iteration=26000, loss=3447.032498741839\n",
      "Current iteration=27000, loss=3446.9828731443067\n",
      "Current iteration=28000, loss=3446.934344069119\n",
      "Current iteration=29000, loss=3446.8868756005622\n",
      "Current iteration=30000, loss=3446.8404330862254\n",
      "Current iteration=31000, loss=3446.7949830960583\n",
      "Current iteration=32000, loss=3446.7504933835758\n",
      "Current iteration=33000, loss=3446.706932848302\n",
      "Current iteration=34000, loss=3446.664271499016\n",
      "Current iteration=35000, loss=3446.622480417607\n",
      "Current iteration=36000, loss=3446.581531723498\n",
      "Current iteration=37000, loss=3446.541398538634\n",
      "Current iteration=38000, loss=3446.502054953091\n",
      "Current iteration=39000, loss=3446.463475991336\n",
      "Current iteration=40000, loss=3446.425637579211\n",
      "Current iteration=41000, loss=3446.388516511667\n",
      "Current iteration=42000, loss=3446.352090421294\n",
      "Current iteration=43000, loss=3446.316337747681\n",
      "Current iteration=44000, loss=3446.281237707626\n",
      "Current iteration=45000, loss=3446.246770266216\n",
      "Current iteration=46000, loss=3446.212916108778\n",
      "Current iteration=47000, loss=3446.179656613722\n",
      "Current iteration=48000, loss=3446.1469738262635\n",
      "Current iteration=49000, loss=3446.114850433023\n",
      "Current iteration=50000, loss=3446.083269737503\n",
      "Current iteration=51000, loss=3446.052215636418\n",
      "Current iteration=52000, loss=3446.0216725968753\n",
      "Current iteration=53000, loss=3445.9916256343854\n",
      "Current iteration=54000, loss=3445.962060291687\n",
      "Current iteration=55000, loss=3445.932962618363\n",
      "Current iteration=56000, loss=3445.9043191512305\n",
      "Current iteration=57000, loss=3445.8761168954857\n",
      "Current iteration=58000, loss=3445.8483433065776\n",
      "Current iteration=59000, loss=3445.8209862727863\n",
      "Current iteration=60000, loss=3445.7940340984974\n",
      "Current iteration=61000, loss=3445.767475488129\n",
      "Current iteration=62000, loss=3445.7412995307027\n",
      "Current iteration=63000, loss=3445.7154956850372\n",
      "Current iteration=64000, loss=3445.6900537655315\n",
      "Current iteration=65000, loss=3445.664963928529\n",
      "Current iteration=66000, loss=3445.640216659225\n",
      "Current iteration=67000, loss=3445.61580275912\n",
      "Current iteration=68000, loss=3445.5917133339694\n",
      "Current iteration=69000, loss=3445.567939782236\n",
      "Current iteration=70000, loss=3445.5444737840057\n",
      "Current iteration=71000, loss=3445.5213072903625\n",
      "Current iteration=72000, loss=3445.4984325131977\n",
      "Current iteration=73000, loss=3445.47584191543\n",
      "Current iteration=74000, loss=3445.453528201632\n",
      "Current iteration=75000, loss=3445.4314843090365\n",
      "Current iteration=76000, loss=3445.4097033989133\n",
      "Current iteration=77000, loss=3445.3881788482913\n",
      "Current iteration=78000, loss=3445.366904242027\n",
      "Current iteration=79000, loss=3445.3458733651887\n",
      "Current iteration=80000, loss=3445.3250801957524\n",
      "Current iteration=81000, loss=3445.3045188975952\n",
      "Current iteration=82000, loss=3445.284183813768\n",
      "Current iteration=83000, loss=3445.264069460048\n",
      "Current iteration=84000, loss=3445.244170518741\n",
      "Current iteration=85000, loss=3445.224481832738\n",
      "Current iteration=86000, loss=3445.2049983998118\n",
      "Current iteration=87000, loss=3445.1857153671363\n",
      "Current iteration=88000, loss=3445.166628026029\n",
      "Current iteration=89000, loss=3445.1477318068974\n",
      "Current iteration=90000, loss=3445.12902227439\n",
      "Current iteration=91000, loss=3445.110495122738\n",
      "Current iteration=92000, loss=3445.0921461712755\n",
      "Current iteration=93000, loss=3445.0739713601406\n",
      "Current iteration=94000, loss=3445.0559667461453\n",
      "Current iteration=95000, loss=3445.0381284987952\n",
      "Current iteration=96000, loss=3445.020452896482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current iteration=97000, loss=3445.002936322805\n",
      "Current iteration=98000, loss=3444.985575263045\n",
      "Current iteration=99000, loss=3444.9683663007686\n",
      "The train data accuracy of the model is  0.23504700940188036 \n",
      "The train data f1 score of the model is  0.4361544172234595\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#degrees = [5,6,7,8,9,10]\n",
    "degrees = [4]\n",
    "k_fold = 2\n",
    "lambdas = np.logspace(-10, 0, 5)\n",
    "\n",
    "for degree in degrees:\n",
    "    cross_validation_demo(y_train, x_train, k_fold, lambdas, degree, seed=154, method=\"RLR\", initial_w=None, max_iters=100000, gamma=1e-30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Prediction of the test data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "degree=9\n",
    "minusloglambda=2\n",
    "lambda_=10**(-loglambda)\n",
    "\n",
    "y_tr_pd, y_te_pd = prediction(x_train, y_train, x_test, degree, lambda_)\n",
    "name=\"submission_{0}_{1}.csv\".format(degree,minusloglambda)\n",
    "create_csv_submission(ids_test, y_te_pd, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The train data accuracy of the model is  0.8246322341956133 \n",
      "The train data f1 score of the model is  0.8158142260188426\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
